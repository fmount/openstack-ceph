#!/bin/env bash

SCRIPT_NAME=$(basename ${BASH_SOURCE[0]})
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
TARGET_BIN=/usr/bin
ORIG_CONFIG="$HOME/bootstrap_ceph.conf"
CONFIG="/etc/ceph/ceph.conf"
KEYRING="/etc/ceph/ceph.client.admin.keyring"
CEPH_PUB_KEY="/etc/ceph/ceph.pub"
ALL_AVAILABLE_DEVICES=0
DEVICES_LIST=("/dev/ceph_vg/ceph_lv_data")
SERVICES=("RGW" "MDS" "NFS") # monitoring is removed for now
SLEEP=5
MIN_OSDS=1

# NFS OPTIONS
FSNAME=${FSNAME:-'cephfs'}
NFS_INGRESS=1
NFS_INGRESS_FPORT=20049
NFS_INGRESS_MPORT=9000
INGRESS_SPEC="ingress.yml"

# POOLS
declare -A POOLS
# POOLS[test]='rbd'
DEFAULT_PG_NUM=8
DEFAULT_PGP_NUM=8

# KEYS


# RGW OPTIONS
RGW_PORT=8080

FSID="4b5c8c0a-ff60-454b-a1b4-9747aa737d19"
IMAGE_PACIFIC=${IMAGE_PACIFIC:-'quay.io/ceph/ceph:v16.2.6'}
IP=192.168.121.205

[ -z "$SUDO" ] && SUDO=sudo

# TODO:
#   - feature1 -> add pv/vg/lv for loopback
#   - install cephadm from centos storage sig


ceph_repo() {
  echo "[centos-ceph-pacific]
  name=centos-ceph-pacific
  baseurl=http://mirror.centos.org/centos/8/storage/x86_64/ceph-pacific/
  gpgcheck=0
  enabled=1" > /etc/yum.repos.d/pacific.repo
}


install_cephadm() {
    curl -O https://raw.githubusercontent.com/ceph/ceph/pacific/src/cephadm/cephadm
    $SUDO mv cephadm $TARGET_BIN
    $SUDO chmod +x $TARGET_BIN/cephadm
    echo "[INSTALL CEPHADM] cephadm is ready"
}

rm_cluster() {
  $SUDO "$CEPHADM" rm-cluster --zap-osds --fsid "$FSID" --force
  echo "[CEPHDM] Cluster deleted"
}

build_osds_from_list() {
  for item in "${DEVICES_LIST[@]}"; do
    echo "Creating osd $item on node $HOSTNAME"
    $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph orch daemon add osd $HOSTNAME:$item
  done
}

rgw() {
  # TODO: Add more logic here and process parameters
  $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph orch apply rgw default \
      '--placement=$HOSTNAME count:1' --port "$RGW_PORT"
}

mds() {
  # TODO: Add more logic here
  # two pools are generated by this action
  # - $FSNAME.FSNAME.data
  # - $FSNAME.FSNAME.meta
  $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph orch apply mds "$FSNAME" \
      --placement="$HOSTNAME"
  $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph fs volume create "$FSNAME" \
      --placement="$HOSTNAME"
}

nfs() {
  echo "[CEPHADM] Deploy nfs.$FSNAME backend"
  $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph orch apply nfs \
      "$FSNAME" --placement="$HOSTNAME"

  if [ "$NFS_INGRESS" -eq 1 ]; then
    echo "[CEPHADM] Deploy nfs.$FSNAME Ingress Service"
    $SUDO "$CEPHADM" shell -m /tmp/"$INGRESS_SPEC" --fsid $FSID \
        --config $CONFIG --keyring $KEYRING -- ceph orch apply -i \
        /mnt/"$INGRESS_SPEC"
fi
}

process_services() {
  for item in "${SERVICES[@]}"; do
    case "$item" in
      "MDS")
        echo "Deploying MDS on node $HOSTNAME"
        mds
        ;;
      "NFS")
        echo "Deploying NFS on node $HOSTNAME"
        nfs
        ;;
      "RGW")
        echo "Deploying RGW on node $HOSTNAME"
        rgw
        ;;
    esac
  done
}

# pools are tied to their application, therefore the function
# iterates over the associative array that defines this relationship
# e.g. { 'volumes': 'rbd', 'manila_data': 'cephfs' }
create_pools() {

  [ "${#POOLS[@]}" -eq 0 ] && return;

  for pool in "${!POOLS[@]}"; do
  $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph osd pool create "$pool" $DEFAULT_PG_NUM \
      $DEFAULT_PGP_NUM replicated --autoscale-mode on

  # set the application to the pool (which also means rbd init the pool)
  $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph osd pool application enable "$pool" ${POOLS[$pool]}
  done
}

build_caps() {
  local CAPS=""
  for pool in "${!POOLS[@]}"; do
    caps="allow rwx pool="$pool
    CAPS+=$caps,
  done
  echo "${CAPS::-1}"
}

create_keys() {

  local name=$1
  local caps
  local osd_caps

  if [ "${#POOLS[@]}" -eq 0 ]; then
      osd_caps="allow *"
  else
      caps=$(build_caps)
      osd_caps="allow class-read object_prefix rbd_children, $caps"
  fi

  current_key=$(sudo cephadm shell ceph auth ls | grep client.***REMOVED***)
  if [ -n "$current_key" ]; then
      # the key exists, we just need to update it
      $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
          --keyring $KEYRING -- ceph auth rm "$name"
  fi

  $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph auth add "$name" mon "allow r" osd "$osd_caps"
}

check_cluster_status() {
  $SUDO "$CEPHADM" shell --fsid $FSID --config $CONFIG \
      --keyring $KEYRING -- ceph -s -f json-pretty
}

install_cephadm

if [ -z "$CEPHADM" ]; then
     CEPHADM=${TARGET_BIN}/cephadm
fi

cat <<EOF > "$ORIG_CONFIG"
[global]
  log to file = true
  osd crush chooseleaf type = 0
  osd_pool_default_pg_num = 8
  osd_pool_default_pgp_num = 8
  osd_pool_default_size = 1
[mon]
  mon_warn_on_insecure_global_id_reclaim_allowed = False
  mon_warn_on_pool_no_redundancy = False
EOF

cluster=$(sudo cephadm ls | jq '.[]' | jq 'select(.name | test("^mon*")).fsid')
if [ -z "$cluster" ]; then
$SUDO $CEPHADM --image "$IMAGE_PACIFIC" \
      bootstrap \
      --fsid $FSID \
      --config "$ORIG_CONFIG" \
      --output-config $CONFIG \
      --output-keyring $KEYRING \
      --output-pub-ssh-key $CEPH_PUB_KEY \
      --allow-overwrite \
      --allow-fqdn-hostname \
      --skip-monitoring-stack \
      --skip-dashboard \
      --skip-firewalld \
      --mon-ip $IP

# Wait cephadm backend to be operational
sleep "$SLEEP"
fi


# let's add some osds
if [ "$ALL_AVAILABLE_DEVICES" -eq 1 ]; then
    $SUDO $CEPHADM shell ceph orch apply osd --all-available-devices
else
    build_osds_from_list
fi


# TODO: Improve this check (waiting for OSD(s))
for f in `seq 1 30`; do
  num_osds=$($SUDO $CEPHADM shell --fsid $FSID --config $CONFIG \
    --keyring $KEYRING -- ceph -s -f json | jq '.osdmap | .num_up_osds')
  if [ "$num_osds" -ge "$MIN_OSDS" ]; then break; fi
  sleep 1
done
echo "[CEPHADM] OSD(s) deployed: $num_osds"

[ "$num_osds" -lt "$MIN_OSDS" ] && exit 255


if [ "$NFS_INGRESS" -eq 1 ]; then
cat > /tmp/$INGRESS_SPEC <<-EOF
service_type: ingress
service_id: nfs.$FSNAME
placement:
  count: 1
spec:
  backend_service: nfs.$FSNAME
  frontend_port: $NFS_INGRESS_FPORT
  monitor_port: $NFS_INGRESS_MPORT
  virtual_ip: $IP/24" > /tmp/"$INGRESS_SPEC"
EOF
fi

# add the provided pools
create_pools
create_keys "client.***REMOVED***"

# add more services
process_services
check_cluster_status
